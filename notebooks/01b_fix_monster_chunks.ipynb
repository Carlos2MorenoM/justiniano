{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891dbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Colab Notebook for Processing Monster Chunks.\n",
    "\n",
    "This script handles the large document chunks skipped by the main pipeline.\n",
    "It recursively splits them into smaller pieces and generates embeddings.\n",
    "\n",
    "Environment Variables / Secrets Required:\n",
    "- DRIVE_FOLDER_NAME: The name of your DVC remote folder in Google Drive.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper for Secrets ---\n",
    "def get_secret(name: str) -> str:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        return userdata.get(name)\n",
    "    except (ImportError, AttributeError, Exception):\n",
    "        return os.getenv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b67bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DRIVE_FOLDER_NAME = get_secret(\"DRIVE_FOLDER_NAME\")\n",
    "\n",
    "if not DRIVE_FOLDER_NAME:\n",
    "    print(\"âŒ ERROR: Missing 'DRIVE_FOLDER_NAME'.\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mount Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d471a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dependencies ---\n",
    "!pip install sentence-transformers tqdm numpy langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf090c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup & Logic ---\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"Loading BGE-M3 model...\")\n",
    "model = SentenceTransformer('BAAI/bge-m3', device='cuda')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = f\"/content/drive/MyDrive/{DRIVE_FOLDER_NAME}/outputs\"\n",
    "MONSTER_CHUNKS_FILE = os.path.join(OUTPUT_DIR, \"monster_chunks_to_fix.jsonl\")\n",
    "\n",
    "if not os.path.exists(MONSTER_CHUNKS_FILE):\n",
    "    raise FileNotFoundError(f\"Monster chunks file not found at {MONSTER_CHUNKS_FILE}\")\n",
    "\n",
    "# Splitter Config\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "MANUAL_BATCH_SIZE = 32\n",
    "processed_sub_chunks = 0\n",
    "total_monster_chunks_read = 0\n",
    "\n",
    "print(f\"Starting recursive processing...\")\n",
    "\n",
    "try:\n",
    "    with open(MONSTER_CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "        batch_texts = []\n",
    "        batch_chunk_ids = []\n",
    "        \n",
    "        for line in tqdm(f, desc=\"Splitting monster chunks\"):\n",
    "            total_monster_chunks_read += 1\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                original_text = data.get('text', '')\n",
    "                doc_id = data.get('doc_id', 'unknown')\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            sub_chunks = text_splitter.split_text(original_text)\n",
    "            \n",
    "            for i, sub_text in enumerate(sub_chunks):\n",
    "                sub_chunk_id = f\"{doc_id}_monster_sub_{i}\"\n",
    "                \n",
    "                # Checkpointing\n",
    "                batch_num = (processed_sub_chunks) // MANUAL_BATCH_SIZE\n",
    "                vector_file = os.path.join(OUTPUT_DIR, f\"batch_monster_{batch_num}_vectors.npy\")\n",
    "                ids_file = os.path.join(OUTPUT_DIR, f\"batch_monster_{batch_num}_ids.json\")\n",
    "\n",
    "                if os.path.exists(vector_file) and len(batch_texts) == 0:\n",
    "                    try:\n",
    "                        with open(ids_file, 'r', encoding='utf-8') as f_ids:\n",
    "                            processed_sub_chunks += len(json.load(f_ids))\n",
    "                    except:\n",
    "                        processed_sub_chunks += MANUAL_BATCH_SIZE\n",
    "                    continue\n",
    "                \n",
    "                batch_texts.append(sub_text)\n",
    "                batch_chunk_ids.append(sub_chunk_id)\n",
    "\n",
    "                if len(batch_texts) >= MANUAL_BATCH_SIZE:\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    batch_embeds = model.encode(batch_texts, batch_size=MANUAL_BATCH_SIZE, show_progress_bar=False)\n",
    "                    \n",
    "                    np.save(vector_file, batch_embeds)\n",
    "                    with open(ids_file, 'w', encoding='utf-8') as f_out:\n",
    "                        json.dump(batch_chunk_ids, f_out)\n",
    "                    \n",
    "                    tqdm.write(f\"Processed monster batch {batch_num}\")\n",
    "                    processed_sub_chunks += len(batch_texts)\n",
    "                    \n",
    "                    batch_texts = []\n",
    "                    batch_chunk_ids = []\n",
    "                    del batch_embeds\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        # Final Batch\n",
    "        if batch_texts:\n",
    "            print(f\"Processing final monster batch...\")\n",
    "            batch_num = (processed_sub_chunks) // MANUAL_BATCH_SIZE\n",
    "            vector_file = os.path.join(OUTPUT_DIR, f\"batch_monster_{batch_num}_vectors.npy\")\n",
    "            ids_file = os.path.join(OUTPUT_DIR, f\"batch_monster_{batch_num}_ids.json\")\n",
    "\n",
    "            if not os.path.exists(vector_file):\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_embeds = model.encode(batch_texts, batch_size=len(batch_texts), show_progress_bar=True)\n",
    "                np.save(vector_file, batch_embeds)\n",
    "                with open(ids_file, 'w', encoding='utf-8') as f_out:\n",
    "                    json.dump(batch_chunk_ids, f_out)\n",
    "                print(f\"Processed final monster batch {batch_num}\")\n",
    "                processed_sub_chunks += len(batch_texts)\n",
    "            \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Complete. Original chunks: {total_monster_chunks_read}. New sub-chunks: {processed_sub_chunks}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
