{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Colab Notebook for Mass Embedding Generation.\n",
    "\n",
    "This script processes the 'knowledge_chunks.jsonl' file using the BGE-M3 model.\n",
    "It implements batching and checkpointing to handle large datasets on limited hardware (T4/L4 GPUs).\n",
    "\n",
    "Environment Variables / Secrets Required:\n",
    "- DRIVE_FOLDER_NAME: The name of your DVC remote folder in Google Drive.\n",
    "- DVC_FILE_HASH: The specific hash of the knowledge_chunks file (from `dvc push` logs).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper for Secrets Management ---\n",
    "def get_secret(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves a secret value from Google Colab Secrets (preferred) or os.environ.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        return userdata.get(name)\n",
    "    except (ImportError, AttributeError, Exception):\n",
    "        # Fallback to standard environment variable\n",
    "        return os.getenv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & Validation ---\n",
    "# Retrieve configuration from environment/secrets\n",
    "DRIVE_FOLDER_NAME = get_secret(\"DRIVE_FOLDER_NAME\")\n",
    "FILE_HASH = get_secret(\"DVC_FILE_HASH\")\n",
    "\n",
    "# Validate configuration\n",
    "if not DRIVE_FOLDER_NAME or not FILE_HASH:\n",
    "    print(\"❌ ERROR: Missing configuration.\")\n",
    "    print(\"Please set 'DRIVE_FOLDER_NAME' and 'DVC_FILE_HASH' in Colab Secrets or os.environ.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"✅ Configuration loaded for Drive Folder: {DRIVE_FOLDER_NAME}\")\n",
    "print(f\"✅ Target File Hash: {FILE_HASH[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb680446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dependencies ---\n",
    "!pip install sentence-transformers tqdm numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path Setup ---\n",
    "# DVC storage logic: /files/md5/{first_2_chars}/{rest_of_hash}\n",
    "file_hash_prefix = FILE_HASH[:2]\n",
    "file_hash_suffix = FILE_HASH[2:]\n",
    "dvc_file_path = f\"/content/drive/MyDrive/{DRIVE_FOLDER_NAME}/files/md5/{file_hash_prefix}/{file_hash_suffix}\"\n",
    "\n",
    "if os.path.exists(dvc_file_path):\n",
    "    print(f\"SUCCESS: Data file found at:\\n{dvc_file_path}\")\n",
    "    CHUNKS_FILE = dvc_file_path\n",
    "    # Output directory for embeddings and checkpoints\n",
    "    OUTPUT_DIR = f\"/content/drive/MyDrive/{DRIVE_FOLDER_NAME}/outputs\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Embeddings will be saved to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"ERROR: Could not find file at {dvc_file_path}. \"\n",
    "        \"Please verify your DRIVE_FOLDER_NAME and DVC_FILE_HASH secrets.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc88223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding Generation Logic ---\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# PyTorch Memory Optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF set to 'expandable_segments:True'\")\n",
    "\n",
    "print(\"Loading BGE-M3 model onto GPU...\")\n",
    "model = SentenceTransformer('BAAI/bge-m3', device='cuda')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Define Output Paths\n",
    "MONSTER_CHUNKS_FILE = os.path.join(OUTPUT_DIR, \"monster_chunks_to_fix.jsonl\")\n",
    "\n",
    "# Config Parameters\n",
    "MANUAL_BATCH_SIZE = 32\n",
    "MAX_CHUNK_LENGTH = 20000 \n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "batch_texts = []\n",
    "batch_chunk_ids = []\n",
    "\n",
    "print(f\"Starting embedding generation. Batch Size: {MANUAL_BATCH_SIZE}\")\n",
    "\n",
    "try:\n",
    "    with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Processing chunks\"):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                text = data.get('text', '')\n",
    "                chunk_id = data.get('chunk_id', 'unknown')\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # Checkpointing: Skip processed batches\n",
    "            batch_num = (processed_count + skipped_count) // MANUAL_BATCH_SIZE\n",
    "            vector_file = os.path.join(OUTPUT_DIR, f\"batch_{batch_num}_vectors.npy\")\n",
    "            ids_file = os.path.join(OUTPUT_DIR, f\"batch_{batch_num}_ids.json\")\n",
    "\n",
    "            if os.path.exists(vector_file):\n",
    "                if len(batch_texts) == 0: \n",
    "                    processed_count += MANUAL_BATCH_SIZE\n",
    "                continue\n",
    "            \n",
    "            # Guardrail: Filter Monster Chunks\n",
    "            if len(text) > MAX_CHUNK_LENGTH:\n",
    "                tqdm.write(f\"SKIPPING monster chunk: {chunk_id} ({len(text)} chars)\")\n",
    "                skipped_count += 1\n",
    "                with open(MONSTER_CHUNKS_FILE, 'a', encoding='utf-8') as monster_f:\n",
    "                    monster_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "            \n",
    "            batch_texts.append(text)\n",
    "            batch_chunk_ids.append(chunk_id)\n",
    "\n",
    "            # Process Batch\n",
    "            if len(batch_texts) >= MANUAL_BATCH_SIZE:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                batch_embeds = model.encode(batch_texts, batch_size=len(batch_texts), show_progress_bar=False)\n",
    "                \n",
    "                np.save(vector_file, batch_embeds)\n",
    "                with open(ids_file, 'w', encoding='utf-8') as f_ids:\n",
    "                    json.dump(batch_chunk_ids, f_ids)\n",
    "                \n",
    "                tqdm.write(f\"Processed and saved batch {batch_num}\")\n",
    "                processed_count += len(batch_texts)\n",
    "                \n",
    "                batch_texts = []\n",
    "                batch_chunk_ids = []\n",
    "                del batch_embeds\n",
    "\n",
    "    # Final Batch Processing\n",
    "    if batch_texts:\n",
    "        print(f\"Processing final batch...\")\n",
    "        batch_num = (processed_count + skipped_count) // MANUAL_BATCH_SIZE\n",
    "        vector_file = os.path.join(OUTPUT_DIR, f\"batch_{batch_num}_vectors.npy\")\n",
    "        ids_file = os.path.join(OUTPUT_DIR, f\"batch_{batch_num}_ids.json\")\n",
    "\n",
    "        if not os.path.exists(vector_file):\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_embeds = model.encode(batch_texts, batch_size=len(batch_texts), show_progress_bar=True)\n",
    "            \n",
    "            np.save(vector_file, batch_embeds)\n",
    "            with open(ids_file, 'w', encoding='utf-8') as f_ids:\n",
    "                json.dump(batch_chunk_ids, f_ids)\n",
    "            \n",
    "            print(f\"Processed final batch {batch_num}\")\n",
    "            processed_count += len(batch_texts)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Complete. Processed: {processed_count}, Skipped: {skipped_count}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610cbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
