"""
Evaluation Service using Ragas.

This module implements an 'LLM-as-a-Judge' evaluator using local Ollama models
to assess the quality of RAG interactions. It calculates metrics such as
Faithfulness and Answer Relevancy to ensure the agent's reliability.
"""
import logging
from typing import List, Dict
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from datasets import Dataset
from langchain_ollama import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings

# Configure logger for this module
log = logging.getLogger(__name__)

# Configuration for the Judge LLM
# We use a lightweight but capable model (Llama 3.1 8B) for evaluation tasks
# to maintain the "Zero Cost" philosophy while ensuring reasonable accuracy.
JUDGE_MODEL_NAME = "llama3.1:8b" 

class EvaluationService:
    """
    Service responsible for running quality evaluations on RAG traces.
    """

    def __init__(self):
        """
        Initializes the Ragas evaluator components with local Ollama models.
        
        Sets up:
        1. The Judge LLM (Critic) for reasoning tasks.
        2. The Embeddings model required for vector-based metrics.
        """
        try:
            # 1. Initialize the Judge LLM (Critic)
            self.judge_llm = ChatOllama(model=JUDGE_MODEL_NAME)
            
            # 2. Initialize Embeddings for evaluation
            # Some Ragas metrics require embeddings to calculate semantic similarity.
            # We reuse the same local model via Ollama to avoid external dependencies.
            self.embeddings = OllamaEmbeddings(model=JUDGE_MODEL_NAME)
            
            log.info(f"Evaluation Service initialized successfully with Judge: {JUDGE_MODEL_NAME}")
        except Exception as e:
            log.error(f"Failed to initialize Evaluation Service: {e}")
            # Set to None to handle graceful degradation in evaluation methods
            self.judge_llm = None

    def evaluate_interaction(
        self, 
        query: str, 
        response: str, 
        retrieved_contexts: List[str]
    ) -> Dict[str, float]:
        """
        Evaluates a single RAG interaction (Trace) using Ragas metrics.

        Args:
            query (str): The user's original question.
            response (str): The answer generated by the Justiniano agent.
            retrieved_contexts (List[str]): A list of text chunks retrieved from 
                                          the vector database (Qdrant) used as context.

        Returns:
            Dict[str, float]: A dictionary containing the calculated scores (0.0 to 1.0)
                              for each metric (e.g., 'faithfulness', 'answer_relevancy').
                              Returns an empty dict if evaluation fails.
        """
        # Fail fast if the service wasn't initialized correctly
        if not self.judge_llm:
            log.warning("Skipping evaluation: Judge LLM is not initialized.")
            return {}

        log.info("⚖️ Starting Ragas Evaluation for current interaction...")

        # 1. Prepare data in HuggingFace Dataset format
        # Ragas expects a dataset structure with specific column names.
        data = {
            'question': [query],
            'answer': [response],
            'contexts': [retrieved_contexts],
            # 'ground_truth': ... (Not available for live chat interactions)
        }
        dataset = Dataset.from_dict(data)

        # 2. Define the metrics to evaluate
        # - Faithfulness: Checks if the answer is grounded in the provided context (Hallucination detection).
        # - Answer Relevancy: Checks if the answer actually addresses the user's prompt.
        metrics = [faithfulness, answer_relevancy]

        try:
            # 3. Execute the evaluation
            # This process involves multiple calls to the LLM and can take several seconds.
            results = evaluate(
                dataset=dataset,
                metrics=metrics,
                llm=self.judge_llm,
                embeddings=self.embeddings,
                raise_exceptions=False, # Prevent crashing the main thread on eval errors
            )

            # 4. Format and extract results
            # 'results' is a Result object; convert to pandas to easily extract the first row.
            scores = results.to_pandas().iloc[0].to_dict()
            
            # Clean up the result dictionary (keep only metrics, remove input data)
            final_metrics = {
                "faithfulness": round(scores.get("faithfulness", 0.0), 2),
                "answer_relevancy": round(scores.get("answer_relevancy", 0.0), 2)
            }
            
            log.info(f"✅ Evaluation complete. Scores: {final_metrics}")
            return final_metrics

        except Exception as e:
            log.error(f"❌ Critical error during Ragas evaluation: {e}", exc_info=True)
            return {}

# Singleton instance to be imported by the router
evaluation_service = EvaluationService()