# backend-ml/data_processing/load_vectordb.py
"""
Vector Database Loader Script.

This module is responsible for reading the batched embedding files (.npy) 
and their corresponding metadata/IDs (.json) generated by the offline 
processing pipeline (Colab) and loading them into the local Qdrant vector database.

It handles:
- Connection to the Qdrant instance.
- Initialization of the vector collection (if not exists).
- Robust iteration over batch files with error handling.
- Upserting vectors to Qdrant.
"""

import os
import json
import logging
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.http import models
from tqdm import tqdm

# --- Configuration ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger(__name__)

# Paths
# Resolves to backend-ml/data/outputs/
DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data')
OUTPUTS_DIR = os.path.join(DATA_DIR, 'outputs') 

# Qdrant Configuration
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "boe_legal_docs"
VECTOR_SIZE = 1024 # Matches BGE-M3 output dimension


def init_collection(client: QdrantClient):
    """
    Initializes the Qdrant collection if it does not already exist.

    Args:
        client (QdrantClient): The authenticated Qdrant client instance.
    """
    try:
        client.get_collection(COLLECTION_NAME)
        log.info(f"Collection '{COLLECTION_NAME}' already exists.")
    except Exception:
        log.info(f"Collection '{COLLECTION_NAME}' not found. Creating it...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=VECTOR_SIZE, 
                distance=models.Distance.COSINE
            ),
        )
        log.info(f"Collection '{COLLECTION_NAME}' created successfully.")


def load_data():
    """
    Main execution function.
    
    Iterates through the 'outputs' directory, pairs .npy vector files with 
    their .json ID files, and performs bulk uploads to Qdrant.
    """
    # 1. Connect to Qdrant
    try:
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        init_collection(client)
    except Exception as e:
        log.error(f"Failed to connect to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}. Is the container running? Error: {e}")
        return

    # 2. Locate batch files
    if not os.path.exists(OUTPUTS_DIR):
        log.error(f"Outputs directory not found at {OUTPUTS_DIR}. Please download the data first.")
        return

    files = [f for f in os.listdir(OUTPUTS_DIR) if f.endswith('_vectors.npy')]
    log.info(f"Found {len(files)} batch files to upload in {OUTPUTS_DIR}.")

    total_uploaded = 0

    # 3. Process each batch
    for vector_file in tqdm(files, desc="Uploading batches"):
        # Construct file paths
        # Expected format: "batch_0_vectors.npy" -> base_name: "batch_0"
        base_name = vector_file.replace('_vectors.npy', '')
        ids_file = f"{base_name}_ids.json"
        
        vec_path = os.path.join(OUTPUTS_DIR, vector_file)
        ids_path = os.path.join(OUTPUTS_DIR, ids_file)

        # Validate pair existence
        if not os.path.exists(ids_path):
            log.warning(f"Missing corresponding IDs file for {vector_file}. Skipping batch.")
            continue

        # Load data from disk
        try:
            vectors = np.load(vec_path)
            with open(ids_path, 'r', encoding='utf-8') as f:
                chunk_ids = json.load(f)
        except Exception as e:
            log.error(f"Error reading files for {base_name}: {e}")
            continue

        # Validate data integrity
        if len(vectors) != len(chunk_ids):
            log.error(f"Data mismatch in {base_name}: {len(vectors)} vectors vs {len(chunk_ids)} IDs. Skipping.")
            continue

        # Prepare Payload (Points)
        # Note: In a full production setup, we might load the full text here.
        # For now, we store the 'original_id' to link back to the knowledge_chunks.jsonl
        points = []
        for i, chunk_id in enumerate(chunk_ids):
            # We use a simple integer ID for Qdrant performance, 
            # but keep the semantic ID in the payload.
            points.append(models.PointStruct(
                id=i + total_uploaded, 
                vector=vectors[i].tolist(),
                payload={"original_id": chunk_id} 
            ))

        # Upload to Qdrant
        try:
            client.upsert(
                collection_name=COLLECTION_NAME,
                points=points
            )
            total_uploaded += len(points)
        except Exception as e:
            log.error(f"Failed to upload batch {base_name} to Qdrant: {e}")

    log.info("="*50)
    log.info(f"Upload process completed successfully.")
    log.info(f"Total vectors indexed: {total_uploaded}")
    log.info("="*50)

if __name__ == "__main__":
    load_data()
